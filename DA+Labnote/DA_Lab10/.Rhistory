knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
url <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
census <- read_csv(
url,
col_names = c(
"age", "workclass", "fnlwgt", "education",
"education_1", "marital_status", "occupation", "relationship",
"race", "sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income"
)
) %>%
mutate(income = factor(income))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(mdsr)
library(tidyverse)
library(tidymodels)
url <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
census <- read_csv(
url,
col_names = c(
"age", "workclass", "fnlwgt", "education",
"education_1", "marital_status", "occupation", "relationship",
"race", "sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income"
)
) %>%
mutate(income = factor(income))
glimpse(census)
set.seed(364)
n <- nrow(census)
census_parts <- census %>%
initial_split(prop = 0.8)
train <- census_parts %>%
training()
test <- census_parts %>%
testing()
list(train, test) %>%
map_int(nrow)
train %>%
skim(capital_gain)
test %>%
skim(capital_gain)
mod_null <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(income ~ 1, data = train)
mod_log_1 <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(income ~ capital_gain, data = train)
mod_log_all <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(
income ~ age + workclass + education + marital_status +
occupation + relationship + race + sex +
capital_gain + capital_loss + hours_per_week,
data = train
)
mods <- tibble(
type = c("null", "log_1", "log_all"),
mod = list(mod_null, mod_log_1, mod_log_all)
)
mods <- mods %>%
mutate(
y_train = list(pull(train, income)),
y_test = list(pull(test, income)),
y_hat_train = map(
mod,
~pull(predict(.x, new_data = train, type = "class"), .pred_class)
),
y_hat_test = map(
mod,
~pull(predict(.x, new_data = test, type = "class"), .pred_class)
)
)
mods
mods <- mods %>%
mutate(
accuracy_train = map2_dbl(y_train, y_hat_train, accuracy_vec),
accuracy_test = map2_dbl(y_test, y_hat_test, accuracy_vec),
sens_test =
map2_dbl(y_test, y_hat_test, sens_vec, event_level = "second"),
spec_test =
map2_dbl(y_test, y_hat_test, spec_vec, event_level = "second")
)
mods %>%
select(type, accuracy_train, accuracy_test, sens_test, spec_test)
mods <- mods %>%
mutate(
y_hat_prob_test = map(
mod,
~pull(predict(.x, new_data = test, type = "prob"), `.pred_>50K`)
),
type = fct_reorder(type, sens_test, .desc = TRUE)
)
mods %>%
select(type, y_test, y_hat_prob_test) %>%
unnest(cols = c(y_test, y_hat_prob_test)) %>%
group_by(type) %>%
roc_curve(truth = y_test, y_hat_prob_test, event_level = "second") %>%
autoplot() +
geom_point(
data = mods,
aes(x = 1 - spec_test, y = sens_test, color = type),
size = 3
) +
scale_color_brewer("Model", palette = "Set2")
train_plus <- train %>%
mutate(high_earner = as.integer(income == ">50K"))
ggplot(train_plus, aes(x = capital_gain, y = high_earner)) +
geom_count(
position = position_jitter(width = 0, height = 0.05),
alpha = 0.5
) +
geom_smooth(
method = "glm", method.args = list(family = "binomial"),
color = "dodgerblue", lty = 2, se = FALSE
) +
geom_hline(aes(yintercept = 0.5), linetype = 3) +
scale_x_log10(labels = scales::dollar)
mod_hours_age <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(income ~ hours_per_week + age, data = train)
broom::tidy(mod_hours_age)
library(MASS)
lda.model = lda(income ~ hours_per_week + age, data = train)
ggplot(train, aes(x = hours_per_week, y = age)) +
geom_point(
aes(color = income)
) +
geom_abline(
intercept= 4.85/0.042, slope=-0.047/0.042, size = 1.5, color = 'grey'
) +
geom_abline(
intercept= 4.59/0.053, slope=-0.058/0.053, size = 1.5, color = 'black'
)
library(NHANES)
people <- NHANES %>%
dplyr::select(Age, Gender, Diabetes, BMI, HHIncome, PhysActive) %>%
drop_na()
glimpse(people)
people %>%
group_by(Diabetes) %>%
count() %>%
mutate(pct = n / nrow(people))
library(modelr)
num_points <- 100
fake_grid <- data_grid(
people,
Age = seq_range(Age, num_points),
BMI = seq_range(BMI, num_points)
)
dmod_null <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(Diabetes ~ 1, data = people)
dmod_log_1 <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(Diabetes ~ Age, data = people)
dmod_log_2 <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(Diabetes ~ BMI, data = people)
dmod_log_12 <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(Diabetes ~ Age + BMI, data = people)
bmi_mods <- tibble(
type = factor(
c("Null", "Logistic (Age)", "Logistic (BMI)", "Logistic (Age, BMI)")
),
mod = list(dmod_null, dmod_log_1, dmod_log_2, dmod_log_12),
y_hat = map(mod, predict, new_data = fake_grid, type = "prob")
)
bmi_mods <- bmi_mods %>%
mutate(
X = list(fake_grid),
yX = map2(y_hat, X, bind_cols)
)
res <- bmi_mods %>%
dplyr::select(type, yX) %>%
unnest(cols = yX)
res
ggplot(data = res, aes(x = Age, y = BMI)) +
geom_tile(aes(fill = .pred_Yes), color = NA) +
geom_count(
data = people,
aes(color = Diabetes), alpha = 0.4
) +
scale_fill_gradient("Prob of\nDiabetes", low = "white", high = "red") +
scale_color_manual(values = c("gold", "black")) +
scale_size(range = c(0, 2)) +
scale_x_continuous(expand = c(0.02, 0)) +
scale_y_continuous(expand = c(0.02, 0)) +
facet_wrap(~fct_rev(type))
summary(lda.model)
sum_lda <- summary(lda.model)
View(lda.model)
lda.model$means
lda.model$scaling
