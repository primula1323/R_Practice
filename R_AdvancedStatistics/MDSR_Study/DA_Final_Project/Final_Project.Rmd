---
title: "Final_Project"
author: "Na SeungChan"
date: "`r Sys.Date()`"
mainfont : NanumGothic
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(caret)
library(splines)#Splines
library(pscl)# zero-inflated models 
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
library(klaR)
library(nnet)
```


# Final Project

regression or classification model을 생성하여 training datasets으로 학습 후에 test datasets으로 평가할 예정입니다.

* 압축 파일에는 dataset1 ~ 20 각각 train/test set 총 40개의 csv 파일과 답을 입력하셔야 하는 test_prediction.csv 파일, 총 41개의 csv파일이 있습니다.

* 각 dataset은 train dataset (n=800)과 test dataset (n=200)으로 구성되어 있으며 train dataset은 outcome Y가 주어지지만, test dataset의 Y는 주어지지 않습니다.

* 각 train dataset의 Y 값을 확인 하시고 (binary인지, count data인지, 연속형 변수인지) 그에 맞는 모델을 사용하셔서 Y도 같은 변수로 예측하시기 바랍니다. (즉, 분류 문제의 경우 확률을 예측하는 것이 아니라, 실제 분류를 진행하셔야 합니다.)

* 여러분은 각 test dataset의 1~200번째 row의 predicted Y의 값을 "test_prediction.csv" 파일의 dataset과 번호가 맞는 column에 입력하시면 됩니다. (ex. dataset18_test의 prediction은 y18 column에 순서대로 채워주시면 됩니다.)

* test_prediction.csv 파일에는 기타 설명 등을 전혀 쓰지말고, (row의 순서대로) prediction/classification한 Y의 값만 입력해주시기 바랍니다. (ex. binary인 경우 각 cell에 0/1만 입력)


최종 제출물은

* (1) 자신이 세운 모델과 그에 대한 설명 파일 (ex. linear regression을 사용하였고, 어떤 변수를 사용하였다.. 정도로 설명하시면 됩니다.)

* (2) 실제 prediction/classification에 사용한 R code

* (3) 각 test dataset의 prediction/classification의 결과를 포함한 "test_prediction.csv" 파일 (200 row, 20 column)

세 개를 제출하여 주시면 됩니다.



# 공통 아이디어


Test Set은 사용할 수 없는 데이터로 생각하고 전혀 고려하지 않는다. Test Set의 x를 잘 관찰하면 x의 range에 대한 정보를 새로 얻을 수 있게 된다든가 하는 일이 벌어질 수도 있을 것이고, train set에서는 binary였던 x가 Test Set에서는 continuous인 악질적인 일이 있을 수도 있지만... 그런 일이 있다면 눈에 띌 것이다. 그리고, extrapolation은 모델 설정 단계부터 다항식 회귀를 적절히 penalize하는 등 방식으로 고려하는 것이 더 타당하지 Test set만 보고 해당 정보를 끌어내려 하는 것은 득보다 실이 더 크다고 판단하였다.


이런 관점에서, 다시 Train Set의 90%만 학습에 활용한다. (이후 'T set'은 학습에 활용된 90%의 Set을, 'V Set'은 나머지 10%의 Set이다.) 각 데이터셋마다 여러 모델을 fitting해 보았고, 여러 모델들 중 V Set에서의 MSE가 가장 작은 모델을 최종 모델로 선정하였다.


그리고, Binary outcome을 제출하는데 MSE로 평가된다는 것은 Accuracy로 평가한다는 것과 완벽히 동일하다. 따라서 이와 같은 문제에서 민감도와 특이도 등에 대해 고려하는 것은 우선순위에서 미룬다.


각 모델을 학습하는 과정에서 하이퍼파라미터를 결정해야 할 때는 Validation Set Approach를 최대한 피하고, k-fold approach 혹은 LOOCV를 최대한 활용하여 T set 내의 모든 정보를 활용하였다.


```{r}
final_csv <- read.csv("./dataset/test_prediction.csv")
colnames_final <- colnames(final_csv)
```



# dataset 1


```{r}
df1_train <- read.csv("./dataset/dataset1_train.csv")
df1_test <- read.csv("./dataset/dataset1_test.csv")
df1_train
summary(df1_train$y)

set.seed(42)
data_split_df1 <- initial_split(df1_train, prop = 0.9)
trn1 <- training(data_split_df1)
tst1 <- testing(data_split_df1)
```


우선 y는 연속형 반응변수이므로 이에 맞는 모델을 사용하면 된다. 그런데... 무슨 모델을 사용하는 것이 좋은지 직관적으로 전혀 알 수가 없다. 우선 lm을 시도해 본다. 


```{r}
best_select_df1 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10)
sum_lm1 <- summary(best_select_df1)

which.min(sum_lm1$cp)
which.min(sum_lm1$bic)
which.min(sum_lm1$adjr2)
```


test error 추정함수를 뭐로 쓰느냐에 따라 모델이 달라지는 것부터 불안하다. 산점도행렬을 확인하면 다음과 같다. 산점도행렬을 처음에 안 쓰는 이유는... 처음에 쓰면 직관적으로 그 어떤 것도 확인할 수 없는 그래프가 되어 버린다.


```{r}
pairs(~x2 + x5 + x7 + x9 + x10 + y, data = trn1)
pairs(~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + y, data = trn1)
ggcorrplot(cor(trn1), lab = T, lab_size = 2)
```


x7에 대한 선형모형이 되어야 함은 확실해 보인다. 그리고... x9와 x10에 대해 이차함수적 관계를 y가 보이는 것 같기도 하니 이를 반영하는 모델도 fitting해 본다. 단, 다항회귀를 막 하면 오차가 발생하는 것을 생각하여 extrapolation이 발생하지 않도록 하고, x7에 대해 선형모형이 아닌 다른 모형을 적합하는 것은 오히려 variance를 발생시켜 test MSE를 늘릴 것이다. 이를 고려하는 회귀모형 적합은 GAM과 natural splines를 통해 가능하다.



```{r}
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 1)

lm1_df1 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x7, data = trn1)

MSE_df1_lm1 <- mean(pluck((predict(lm1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm1


coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 4)

lm2_df1 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x2 + x5 + x7 + x9, data = trn1)

MSE_df1_lm2 <- mean(pluck((predict(lm2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm2


coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 5)

lm3_df1 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x2 + x5 + x7 + x9 + x10, data = trn1)

MSE_df1_lm3 <- mean(pluck((predict(lm3_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm3
```



```{r}
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam1_df1)
summary(gam2_df1)

MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam2
```



lm 이외 penalized regression에 해당하는 다른 모델을 써 본다. 어떤 변수를 사용해야 할지 불분명하므로 LASSO를 사용하되, lambda의 값은 k-fold cross validation으로 결정한다.


```{r}
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df1, trn1$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df1 <- glmnet(X_df1, trn1$y, alpha = 1, lambda = bestlambda)
lasso_df1
MSE_df1_lasso <- mean((predict.glmnet(lasso_df1, Xtst_df1) - tst1$y)^2)
MSE_df1_lasso
```


Basic lm에 비해 MSE의 압도적 개선이 있었다. 그러나 gam model에 비해서는 MSE가 낮은데, 이는 LASSO가 linear regression에 penalty를 더한 모델이기 때문이다. Ridge를 사용하면 값은 어떻게 되는가?


```{r}
set.seed(42)
cv.out <- cv.glmnet(X_df1, trn1$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df1 <- glmnet(X_df1, trn1$y, alpha = 0, lambda = bestlambda)
ridge_df1
MSE_df1_ridge <- mean((predict(ridge_df1, Xtst_df1) - tst1$y)^2)
MSE_df1_ridge
```



```{r}
MSE_df1_lm1
MSE_df1_lm2
MSE_df1_lm3
MSE_df1_gam1
MSE_df1_gam2
MSE_df1_ridge
MSE_df1_lasso
```


따라서 gam2 model을 사용한다. 이후의 연속형 변수에서도 이와 같은 논리의 흐름으로 분석이 진행되며, 분석의 기초 흐름의 설명은 다수 생략된다.


```{r}
final_csv[,2] <- predict(gam2_df1, newdata = df1_test)
```



# dataset 2


```{r}
df2_train <- read.csv("./dataset/dataset2_train.csv")
df2_test <- read.csv("./dataset/dataset2_test.csv")
df2_train
summary(df2_train)

sum(df2_train$y)/800
ggcorrplot(cor(df2_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df2 <- initial_split(df2_train, prop = 0.9)
trn2 <- training(data_split_df2)
tst2 <- testing(data_split_df2)
```


우선 y가 Binary variable이므로 이에 맞춰 logistic regression, naive bayse, lda, qda 등 분류 모델에 적합한 다양한 방식을 생각할 수 있다. 한편 y = 1의 비율은 0.67875 가량이고, x4와 x5, x10이 두드러지게 y와 상관계수가 높은 것을 쉽게 볼 수 있다. x6도 애매하고... 일단 가장 간단하게 logistic regression을 선택하는데, 당연히 predictor를 싸그리 선택하면 V set MSE 퍼포먼스가 낮아질 것이고 penalized logistic regression을 선택한다.


```{r}
pairs(~x2 + x4 + x5 + x6 + x10 + y, data = trn2)
```



```{r}
X_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)[,-1]
y_df2 <- trn2$y
Xtst_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst2)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df2
coef(lasso_df2)

pred <- ifelse(predict(lasso_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_lasso <- mean((pred - tst2$y)^2)
MSE_df2_lasso
```


```{r}
set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df2

pred <- ifelse(predict(ridge_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_ridge <- mean((pred - tst2$y)^2)
MSE_df2_ridge
```


Naive Bayes를 실험해 본다. 


```{r}
NBayes1_df2 <- naiveBayes(y ~ x4 + x5 + x10, data = trn2)
NBayes1_df2
MSE_df2_NBayes1 <- mean((as.numeric(predict(NBayes1_df2, tst2, type = "class"))-1-tst2$y)^2)


NBayes2_df2 <- naiveBayes(y ~ x4 + x10, data = trn2)
NBayes2_df2
MSE_df2_NBayes2 <- mean((as.numeric(predict(NBayes2_df2, tst2, type = "class"))-1-tst2$y)^2)


NBayes3_df2 <- naiveBayes(y ~ x4 + x5 + x6 + x10, data = trn2)
NBayes3_df2
MSE_df2_NBayes3 <- mean((as.numeric(predict(NBayes3_df2, tst2, type = "class"))-1-tst2$y)^2)
```



LDA와 QDA는 범주형 변수인 x4, x5를 모델에 포함하기 어려우므로 사용하지 않았다. 특히, LDA를 사용하면 분산이 같지 않은 문제가 있다. 


```{r}
for (i in 1:10) {
  print(paste('Variance of x_', i, '=', var(df2_train)[i+1,i+1], sep = ''))
}
```




```{r}
MSE_df2_lasso
MSE_df2_ridge
MSE_df2_NBayes1
MSE_df2_NBayes2
MSE_df2_NBayes3
```

따라서 penalized logistic model을 사용한다.


```{r}
tstX <- model.matrix(~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df2_test)[,-1]
final_csv[,3] <- ifelse(predict(lasso_df2, newx = tstX, type = "response") > 0.5, 1, 0)[,1]
```



# dataset 3


```{r}
df3_train <- read.csv("./dataset/dataset3_train.csv")
df3_test <- read.csv("./dataset/dataset3_test.csv")
df3_train
summary(df3_train)
pairs(~., data = df3_train)
hist(df3_train$y)

set.seed(42)
data_split_df3 <- initial_split(df3_train, prop = 0.9)
trn3 <- training(data_split_df3)
tst3 <- testing(data_split_df3)
```


우선 y는 연속형 반응변수이므로 이에 맞는 모델을 사용하면 된다. 그런데... 무슨 모델을 사용하는 것이 좋은지 직관적으로 전혀 알 수가 없다. 그런데 y의 자료구조가 좀 이상하다...?


```{r}
df3_train %>% filter(y > 10)
trn3_drop <- trn3 %>% filter(y < 7.4)
```

이와 같이, y>10인 자료가 5개뿐일 정도로 y의 분포가 극단적인 양의 왜도를 가진다. 해당 분포에서 y의 왜도가 얼마나 체계적인지 예측할 수 있는 변수를 찾아낼 수 있다면 best지만, 고작 5개 변수로 그런 게 되기 어렵다... 

어디부터 어디까지가 outlier인지는 분석가의 주관에 달린 영역이고, 해당 분석에서는 편의를 위해 임의로 y > 7.4 이상의 값을 outlier로 보았다. 사실 y=10 등 값이 문턱으로 더 합리적이지만, y = 7.4로 두면 집합 분할 개수가 배수로 딱 맞는 게 이유일 뿐이다. 어차피 마지막에 MSE로 outlier를 뺀 분석과 빼지 않은 분석이 모두 시행되고, outlier를 빼고 fitting한 model과 outlier를 넣고 fitting한 model 중 MSE가 더 작은 모델을 선택할 것이니 아무 문제 없다.


우선 lm을 시도해 본다. model selection 이후 lm을 fitting해 본다.


```{r}
best_select_df3 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10)
sum_lm3 <- summary(best_select_df3)

which.min(sum_lm3$cp)
which.min(sum_lm3$bic)
which.min(sum_lm3$adjr2)

coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10), id = 1)

coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10), id = 2)
```


```{r}
ggcorrplot(cor(trn3), lab = T, lab_size = 2)
pairs(~x6 + x10 + y, data = trn3)
pairs(~x6 + x10 + y, data = trn3_drop)
```


interaction으로 인해 회귀계수 문제가 생긴 부분은 크지 않아 보인다. 



```{r}
lm1_df3 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x6, data = trn3)

MSE_df3_lm1 <- mean(pluck((predict(lm1_df3, tst3) - tst3$y)^2, 1))
MSE_df3_lm1


lm2_df3 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x6 + x10, data = trn3)

MSE_df3_lm2 <- mean(pluck((predict(lm2_df3, tst3) - tst3$y)^2, 1))
MSE_df3_lm2
```

lm 이외 다른 모델을 써 본다. gam을 우선 사용하며, x6의 자료 분포 형태가 선형이 아니므로 natural splines를 적용한다.


```{r}
gam1_df3 <- gam(y ~ ns(x6), data = trn3)
summary(gam1_df3)
gam2_df3 <- gam(y ~ ns(x6) + ns(x10), data = trn3)
summary(gam2_df3)
gam1_df3.drop <- gam(y ~ ns(x6), data = trn3_drop)
summary(gam1_df3.drop)
gam2_df3.drop <- gam(y ~ ns(x6) + ns(x10), data = trn3_drop)
summary(gam2_df3.drop)

MSE_df3_gam1 <- mean(pluck((predict(gam1_df3, newdata = tst3) - tst3$y)^2, 1))
MSE_df3_gam1.drop <- mean(pluck((predict(gam1_df3.drop, newdata = tst3) - tst3$y)^2, 1))
MSE_df3_gam2 <- mean(pluck((predict(gam2_df3, newdata = tst3) - tst3$y)^2, 1))
MSE_df3_gam2.drop <- mean(pluck((predict(gam2_df3.drop, newdata = tst3) - tst3$y)^2, 1))

MSE_df3_gam1
MSE_df3_gam1.drop
MSE_df3_gam2
MSE_df3_gam2.drop
```


LASSO를 사용하되, lambda의 값은 k-fold cross validation으로 결정한다.

```{r}
X_df3 <- model.matrix(y ~ x6 + x10, data = trn3)[,-1]
Xtst_df3 <- model.matrix(y ~ x6 + x10, data = tst3)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df3, trn3$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df3 <- glmnet(X_df3, trn3$y, alpha = 1, lambda = bestlambda)
summary(lasso_df3)
MSE_df3_lasso <- mean((predict(lasso_df3, Xtst_df3) - tst3$y)^2)
MSE_df3_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df3, trn3$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df3 <- glmnet(X_df3, trn3$y, alpha = 0, lambda = bestlambda)
summary(ridge_df3)
MSE_df3_ridge <- mean((predict.glmnet(ridge_df3, Xtst_df3) - tst3$y)^2)
MSE_df3_ridge
```




```{r}
MSE_df3_lm1
MSE_df3_lm2
MSE_df3_gam1
MSE_df3_gam1.drop
MSE_df3_gam2
MSE_df3_gam2.drop
MSE_df3_ridge
MSE_df3_lasso
```

와 같으므로 gam2 model을 사용한다.


```{r}
final_csv[,4] <- predict(gam2_df3, newdata = df3_test)
```



# dataset 4


```{r}
df4_train <- read.csv("./dataset/dataset4_train.csv")
df4_test <- read.csv("./dataset/dataset4_test.csv")
df4_train
summary(df4_train$y)
ggcorrplot(cor(df4_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df4 <- initial_split(df4_train, prop = 0.9)
trn4 <- training(data_split_df4)
tst4 <- testing(data_split_df4)
```



```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn4)

best_select_df4 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn4, nvmax = 10)
sum_lm4 <- summary(best_select_df4)

which.min(sum_lm4$cp)
which.min(sum_lm4$bic)
which.min(sum_lm4$adjr2)

coef(best_select_df4, id = 1)
coef(best_select_df4, id = 3)
```






```{r}
lm1_df4 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x9, data = trn4)

MSE_df4_lm1 <- mean(pluck((predict(lm1_df4, tst4) - tst4$y)^2, 1))
MSE_df4_lm1

lm2_df4 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x5 + x7 + x9, data = trn4)

MSE_df4_lm2 <- mean(pluck((predict(lm2_df4, tst4) - tst4$y)^2, 1))
MSE_df4_lm2


gam1_df4 <- gam(y ~ ns(x9), data = trn4)
summary(gam1_df4)
gam2_df4 <- gam(y ~ ns(x5) + ns(x7) + ns(x9), data = trn4)
summary(gam2_df4)
gam3_df4 <- gam(y ~ ns(x5) + ns(x7) + x9, data = trn4)
summary(gam3_df4)
MSE_df4_gam1 <- mean(pluck((predict(gam1_df4, newdata = tst4) - tst4$y)^2, 1))
MSE_df4_gam1
MSE_df4_gam2 <- mean(pluck((predict(gam2_df4, newdata = tst4) - tst4$y)^2, 1))
MSE_df4_gam2
MSE_df4_gam3 <- mean(pluck((predict(gam3_df4, newdata = tst4) - tst4$y)^2, 1))
MSE_df4_gam3
```



```{r}
X_df4 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn4)[,-1]
Xtst_df4 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst4)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df4, trn4$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df4 <- glmnet(X_df4, trn4$y, alpha = 1, lambda = bestlambda)
summary(lasso_df4)
MSE_df4_lasso <- mean((predict(lasso_df4, Xtst_df4) - tst4$y)^2)
MSE_df4_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df4, trn4$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df4 <- glmnet(X_df4, trn4$y, alpha = 0, lambda = bestlambda)
summary(ridge_df4)
MSE_df4_ridge <- mean((predict.glmnet(ridge_df4, Xtst_df4) - tst4$y)^2)
MSE_df4_ridge
```



```{r}
MSE_df4_lm1
MSE_df4_lm2
MSE_df4_gam1
MSE_df4_gam2
MSE_df4_gam3
MSE_df4_lasso
MSE_df4_ridge

final_csv[,5] <- predict(gam1_df4, df4_test)
```



# dataset 5


```{r}
df5_train <- read.csv("./dataset/dataset5_train.csv")
df5_test <- read.csv("./dataset/dataset5_test.csv")
df5_train
summary(df5_train$y)
ggcorrplot(cor(df5_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df5 <- initial_split(df5_train, prop = 0.9)
trn5 <- training(data_split_df5)
tst5 <- testing(data_split_df5)
```


```{r}
X_df5 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn5)[,-1]
Xtst_df5 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst5)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df5, trn5$y, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df5 <- glmnet(X_df5, trn5$y, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df5
coef(lasso_df5)

pred <- ifelse(predict(lasso_df5, newx = Xtst_df5, type = "response") > 0.5, 1, 0)
MSE_df5_lasso <- mean((pred - tst5$y)^2)
MSE_df5_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df5, trn5$y, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df5 <- glmnet(X_df5, trn5$y, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df5

pred <- ifelse(predict(ridge_df5, newx = Xtst_df5, type = "response") > 0.5, 1, 0)
MSE_df5_ridge <- mean((pred - tst5$y)^2)
MSE_df5_ridge
```


```{r}
NBayes1_df5 <- naiveBayes(y ~ x1 + x7 + x8, data = trn5)
NBayes1_df5
MSE_df5_NBayes1 <- mean((as.numeric(predict(NBayes1_df5, tst5, type = "class"))-1-tst5$y)^2)
MSE_df5_NBayes1

NBayes2_df5 <- naiveBayes(y ~ x1 + x8, data = trn5)
NBayes2_df5
MSE_df5_NBayes2 <- mean((as.numeric(predict(NBayes2_df5, tst5, type = "class"))-1-tst5$y)^2)
MSE_df5_NBayes2
```






```{r}
MSE_df5_lasso
MSE_df5_ridge
MSE_df5_NBayes1
MSE_df5_NBayes2

tstX <- model.matrix(~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df5_test)[,-1]
final_csv[,6] <- ifelse(predict(lasso_df5, newx = tstX, type = "response") > 0.5, 1, 0)[,1]
```



# dataset 6


```{r}
df6_train <- read.csv("./dataset/dataset6_train.csv")
df6_test <- read.csv("./dataset/dataset6_test.csv")
df6_train
summary(df6_train$y)
ggcorrplot(cor(df6_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df6 <- initial_split(df6_train, prop = 0.9)
trn6 <- training(data_split_df6)
tst6 <- testing(data_split_df6)
```


```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn6)

best_select_df6 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn6, nvmax = 10)
sum_lm6 <- summary(best_select_df6)

which.min(sum_lm6$cp)
which.min(sum_lm6$bic)
which.min(sum_lm6$adjr2)

coef(best_select_df6, id = 1)
coef(best_select_df6, id = 2)
coef(best_select_df6, id = 10)


lm1_df6 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x3, data = trn6)

MSE_df6_lm1 <- mean(pluck((predict(lm1_df6, tst6) - tst6$y)^2, 1))
MSE_df6_lm1

lm2_df6 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x3 + x9, data = trn6)

MSE_df6_lm2 <- mean(pluck((predict(lm2_df6, tst6) - tst6$y)^2, 1))
MSE_df6_lm2

lm3_df6 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn6)

MSE_df6_lm3 <- mean(pluck((predict(lm3_df6, tst6) - tst6$y)^2, 1))
MSE_df6_lm3
```



```{r}
gam.full_df6 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn6)
summary(gam.full_df6)

gam1_df6 <- gam(y ~ x3, data = trn6)
summary(gam1_df6)
gam2_df6 <- gam(y ~ x3 + ns(x9), data = trn6)
summary(gam2_df6)

MSE_df6_gam1 <- mean(pluck((predict(gam1_df6, newdata = tst6) - tst6$y)^2, 1))
MSE_df6_gam1
MSE_df6_gam2 <- mean(pluck((predict(gam2_df6, newdata = tst6) - tst6$y)^2, 1))
MSE_df6_gam2
```


```{r}
X_df6 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn6)[,-1]
Xtst_df6 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst6)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df6, trn6$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df6 <- glmnet(X_df6, trn6$y, alpha = 1, lambda = bestlambda)
summary(lasso_df6)
MSE_df6_lasso <- mean((predict(lasso_df6, Xtst_df6) - tst6$y)^2)
MSE_df6_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df6, trn6$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df6 <- glmnet(X_df6, trn6$y, alpha = 0, lambda = bestlambda)
summary(ridge_df6)
MSE_df6_ridge <- mean((predict.glmnet(ridge_df6, Xtst_df6) - tst6$y)^2)
MSE_df6_ridge
```



```{r}
MSE_df6_lm1
MSE_df6_lm2
MSE_df6_lm3
MSE_df6_gam1
MSE_df6_gam2
MSE_df6_lasso
MSE_df6_ridge

tstX <- model.matrix(~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df6_test)[, -1]
final_csv[,7] <- predict(lasso_df6, newx = tstX)[,1]
```



# dataset 7


```{r}
df7_train <- read.csv("./dataset/dataset7_train.csv")
df7_test <- read.csv("./dataset/dataset7_test.csv")
df7_train
summary(df7_train$y)
ggcorrplot(cor(df7_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df7 <- initial_split(df7_train, prop = 0.9)
trn7 <- training(data_split_df7)
tst7 <- testing(data_split_df7)
```


response variable y는 count variable이다. poisson regression, negative binomial regression, zero-inflated poisson regression, zero-inflated negative binomial regression을 고려하자.


```{r}
mean(df7_train$y)
var(df7_train$y)
hist(df7_train$y)
```


overdispersed. 순수 poisson regression은 적절하지 않다.



```{r, eval = FALSE}
poisson_df7_full <- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df7_train, family = 'poisson')
summary(poisson_df7_full)
cv.err <- cv.glm(df7_train[,-1], poisson_df7_full)
cv.err$delta
```


full model에서 몇몇 계수들은 영향력이 크고, 몇몇 계수들은 영향력이 거의 없는 것으로 나오므로 이를 제외하고 reduced model을 fitting해 본다.



```{r, eval=FALSE}
poisson_df7_reduced <- glm(y ~ x2 + x7 + x8 + x10, data = df7_train, family = 'poisson')
summary(poisson_df7_reduced)
cv.err <- cv.glm(df7_train[,-1], poisson_df7_reduced)
cv.err$delta
```


cv.error의 0.5 가량 감소가 있었다. 이로부터 poisson regression을 사용하는 경우, reduced model을 사용하는 것이 적절함을 안다. 



```{r}
negbin_df7_full <- glm.nb(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df7_train)
summary(negbin_df7_full)
cv.err <- cv.glm(df7_train[,-1], negbin_df7_full)
cv.err$delta
```


full model에서 몇몇 계수들은 영향력이 크고, 몇몇 계수들은 영향력이 거의 없는 것으로 나오므로 이를 제외하고 reduced model을 fitting해 본다.


```{r}
negbin_df7_reduced <- glm.nb(y ~ x2 + x7 + x8 + x10, data = df7_train)
summary(negbin_df7_reduced)
cv.err <- cv.glm(df7_train[,-1], negbin_df7_reduced)
cv.err$delta
```


cv.error의 0.18 가량 감소가 있었다. 이로부터 negative binomial regression을 사용하는 경우, reduced model을 사용하는 것이 적절함을 안다. 


```{r}
k <- round(predict(negbin_df7_reduced, newdata = df7_test))
for (i in 1:200) {
  if (k[i] < 0) {
    k[i] <- 0
  }
}
k
```


```{r}
zipo1_df7 <- zeroinfl(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn7)
summary(zipo1_df7)
MSE_df7_zipo1 <- mean((round(predict(zipo1_df7, newdata = tst7, type = 'count')) - tst7$y)^2)

zipo2_df7 <- zeroinfl(y ~ x1 + x2 + x7 + x8 + x10, data = trn7)
summary(zipo2_df7)
MSE_df7_zipo2 <- mean((round(predict(zipo2_df7, newdata = tst7, type = 'count')) - tst7$y)^2)

zipo3_df7 <- zeroinfl(y ~ x2 + x7 + x8 + x10, data = trn7)
summary(zipo3_df7)
MSE_df7_zipo3 <- mean((round(predict(zipo3_df7, newdata = tst7, type = 'count')) - tst7$y)^2)

zinegbin1_df7 <- zeroinfl(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn7)
summary(zinegbin1_df7)
MSE_df10_zinegbin1 <- mean((round(predict(zinegbin1_df7, newdata = tst7, type = 'count')) - tst7$y)^2)

zinegbin2_df7 <- zeroinfl(y ~ x2 + x7 + x8 + x10, data = trn7)
summary(zinegbin2_df7)
MSE_df10_zinegbin2 <- mean((round(predict(zinegbin2_df7, newdata = tst7, type = 'count')) - tst7$y)^2)
```


```{r}
MSE_df7_zipo1 
MSE_df7_zipo2 
MSE_df7_zipo3 
MSE_df10_zinegbin1
MSE_df10_zinegbin2

final_csv[,8] <- round(predict(zinegbin1_df7, newdata = df7_test, type = 'count'))
```



# dataset 8


```{r}
df8_train <- read.csv("./dataset/dataset8_train.csv")
df8_test <- read.csv("./dataset/dataset8_test.csv")
df8_train
summary(df8_train$y)
ggcorrplot(cor(df8_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df8 <- initial_split(df8_train, prop = 0.9)
trn8 <- training(data_split_df8)
tst8 <- testing(data_split_df8)
```


interaction이 정말 강하게 보이는데... 이걸 안 넣고 model을 fitting하는 게 말이 안 되어 보인다.



```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn8)

best_select_df8 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6*x7*x8*x9*x10, data = trn8)
sum_lm8 <- summary(best_select_df8)
coef(best_select_df8, id = which.min(sum_lm8$cp))
coef(best_select_df8, id = which.min(sum_lm8$bic))
coef(best_select_df8, id = which.min(sum_lm8$adjr2))
```


```{r}
lm1_df8 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x3 + x6 + x7 + x9 + x10, data = trn8)

MSE_df8_lm1 <- mean(pluck((predict(lm1_df8, tst8) - tst8$y)^2, 1))
MSE_df8_lm1

lm2_df8 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x6 + x7 + x9 + x10, data = trn8)

MSE_df8_lm2 <- mean(pluck((predict(lm2_df8, tst8) - tst8$y)^2, 1))
MSE_df8_lm2

lm3_df8 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x10, data = trn8)

MSE_df8_lm3 <- mean(pluck((predict(lm3_df8, tst8) - tst8$y)^2, 1))
MSE_df8_lm3
```

```{r}
X_df8 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6*x7*x8*x9*x10, data = trn8)[,-1]
Xtst_df8 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6*x7*x8*x9*x10, data = tst8)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df8, trn8$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df8 <- glmnet(X_df8, trn8$y, alpha = 1, lambda = bestlambda)
summary(lasso_df8)
MSE_df8_lasso <- mean((predict(lasso_df8, Xtst_df8) - tst8$y)^2)
MSE_df8_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df8, trn8$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df8 <- glmnet(X_df8, trn8$y, alpha = 0, lambda = bestlambda)
summary(ridge_df8)
MSE_df8_ridge <- mean((predict.glmnet(ridge_df8, Xtst_df8) - tst8$y)^2)
MSE_df8_ridge
```


```{r}
MSE_df8_lm1
MSE_df8_lm2
MSE_df8_lm3
MSE_df8_lasso
MSE_df8_ridge

tstX <- model.matrix(~x1 + x2 + x3 + x4 + x5 + x6*x7*x8*x9*x10, data = df8_test)[, -1]
final_csv[,9] <- predict(ridge_df8, newx = tstX)[,1]
```






# dataset 9


```{r}
df9_train <- read.csv("./dataset/dataset9_train.csv")
df9_test <- read.csv("./dataset/dataset9_test.csv")
df9_train
summary(df9_train$y)
ggcorrplot(cor(df9_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df9 <- initial_split(df9_train, prop = 0.9)
trn9 <- training(data_split_df9)
tst9 <- testing(data_split_df9)
```



```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn9)

best_select_df9 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn9, nvmax = 10)
sum_lm9 <- summary(best_select_df9)
coef(best_select_df9, id = which.min(sum_lm9$cp))
coef(best_select_df9, id = which.min(sum_lm9$bic))
coef(best_select_df9, id = which.min(sum_lm9$adjr2))
```





```{r}
lm1_df9 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x7, data = trn9)

MSE_df9_lm1 <- mean(pluck((predict(lm1_df9, tst9) - tst9$y)^2, 1))
MSE_df9_lm1

lm2_df9 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x1 + x5 + x6 + x7, data = trn9)

MSE_df9_lm2 <- mean(pluck((predict(lm2_df9, tst9) - tst9$y)^2, 1))
MSE_df9_lm2

lm3_df9 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x1 + x5 + x6 + x7 + x9, data = trn9)

MSE_df9_lm3 <- mean(pluck((predict(lm3_df9, tst9) - tst9$y)^2, 1))
MSE_df9_lm3
```



```{r}
gam.full_df9 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn9)
summary(gam.full_df9)
```



```{r}
gam1_df9 <- gam(y ~ x1 + x3 + x5 + ns(x6) + x7, data = trn9)
summary(gam1_df9)
gam2_df9 <- gam(y ~ x1 + x3 + x5 + ns(x6) + ns(x7), data = trn9)
summary(gam2_df9)

MSE_df9_gam1 <- mean(pluck((predict(gam1_df9, newdata = tst9) - tst9$y)^2, 1))
MSE_df9_gam1
MSE_df9_gam2 <- mean(pluck((predict(gam2_df9, newdata = tst9) - tst9$y)^2, 1))
MSE_df9_gam2
```



```{r}
X_df9 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn9)[,-1]
Xtst_df9 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst9)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df9, trn9$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df9 <- glmnet(X_df9, trn9$y, alpha = 1, lambda = bestlambda)
summary(lasso_df9)
MSE_df9_lasso <- mean((predict(lasso_df9, Xtst_df9) - tst9$y)^2)
MSE_df9_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df9, trn9$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df9 <- glmnet(X_df9, trn9$y, alpha = 0, lambda = bestlambda)
summary(ridge_df9)
MSE_df9_ridge <- mean((predict.glmnet(ridge_df9, Xtst_df9) - tst9$y)^2)
MSE_df9_ridge
```


```{r}
MSE_df9_lm1
MSE_df9_lm2
MSE_df9_lm3
MSE_df9_gam1
MSE_df9_gam2
MSE_df9_lasso
MSE_df9_ridge

final_csv[,10] <- predict(gam2_df9, newdata = df9_test)
```





# dataset 10


```{r}
df10_train <- read.csv("./dataset/dataset10_train.csv")
df10_test <- read.csv("./dataset/dataset10_test.csv")
df10_train
summary(df10_train$y)
ggcorrplot(cor(df10_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df10 <- initial_split(df10_train, prop = 0.9)
trn10 <- training(data_split_df10)
tst10 <- testing(data_split_df10)
```


```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = df10_train)
mean(df10_train$y)
var(df10_train$y)
hist(df10_train$y)
```


극단적인 zero-inflated count data.


```{r}
zipo1_df10 <- zeroinfl(y ~ x1 + x2 + x7 + x8 + x9, data = trn10)
summary(zipo1_df10)
MSE_df10_zipo1 <- mean((round(predict(zipo1_df10, newdata = tst10, type = 'count')) - tst10$y)^2)

zipo2_df10 <- zeroinfl(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn10)
summary(zipo2_df10)
MSE_df10_zipo2 <- mean((round(predict(zipo2_df10, newdata = tst10, type = 'count')) - tst10$y)^2)

zipo3_df10 <- zeroinfl(y ~ x1 + x2 + ns(x7) + ns(x8) + ns(x9), data = trn10)
summary(zipo3_df10)
MSE_df10_zipo3 <- mean((round(predict(zipo3_df10, newdata = tst10, type = 'count')) - tst10$y)^2)

zipo4_df10 <- zeroinfl(y ~ x1 + x8 + x9, data = trn10)
summary(zipo4_df10)
MSE_df10_zipo4 <- mean((round(predict(zipo4_df10, newdata = tst10, type = 'count')) - tst10$y)^2)

zinegbin1_df10 <- zeroinfl(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn10)
summary(zinegbin1_df10)
MSE_df10_zinegbin1 <- mean((round(predict(zinegbin1_df10, newdata = tst10, type = 'count')) - tst10$y)^2)
```


```{r}
MSE_df10_zipo1
MSE_df10_zipo2
MSE_df10_zipo3
MSE_df10_zipo4
MSE_df10_zinegbin1

final_csv[,11] <- round(predict(zipo1_df10, newdata = df10_test, type = 'count'))
```



# dataset 11


```{r}
df11_train <- read.csv("./dataset/dataset11_train.csv")
df11_test <- read.csv("./dataset/dataset11_test.csv")
df11_train
summary(df11_train$y)
ggcorrplot(cor(df11_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df11 <- initial_split(df11_train, prop = 0.9)
trn11 <- training(data_split_df11)
tst11 <- testing(data_split_df11)
```


```{r}
X_df11 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn11)[,-1]
Xtst_df11 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst11)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df11, trn11$y, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df11 <- glmnet(X_df11, trn11$y, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df11
coef(lasso_df11)

pred <- ifelse(predict(lasso_df11, newx = Xtst_df11, type = "response") > 0.5, 1, 0)
MSE_df11_lasso <- mean((pred - tst11$y)^2)
MSE_df11_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df11, trn11$y, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df11 <- glmnet(X_df11, trn11$y, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df11

pred <- ifelse(predict(ridge_df11, newx = Xtst_df11, type = "response") > 0.5, 1, 0)
MSE_df11_ridge <- mean((pred - tst11$y)^2)
MSE_df11_ridge
```


```{r}
NBayes_df11 <- naiveBayes(y ~ x2 + x3 + x4 + x5 + x7 + x9 + x10, data = trn11)
NBayes_df11
MSE_df11_NBayes <- mean((as.numeric(predict(NBayes_df11, tst11, type = "class"))-1-tst11$y)^2)
MSE_df11_NBayes
```


```{r}
MSE_df11_lasso
MSE_df11_ridge
MSE_df11_NBayes

final_csv[, 12] <- predict(NBayes_df11, df11_test, type = "class")
```



# dataset 12


```{r}
df12_train <- read.csv("./dataset/dataset12_train.csv")
df12_test <- read.csv("./dataset/dataset12_test.csv")
df12_train
summary(df12_train$y)
ggcorrplot(cor(df12_train), lab = T, lab_size = 2)
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = df12_train)

set.seed(42)
data_split_df12 <- initial_split(df12_train, prop = 0.9)
trn12 <- training(data_split_df12)
tst12 <- testing(data_split_df12)
```


```{r}
X_df12 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn12)[, -1]
Xtst_df12 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = tst12)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df12, trn12$y, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df12 <- glmnet(X_df12, trn12$y, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df12
coef(lasso_df12)

pred <- ifelse(predict(lasso_df12, newx = Xtst_df12, type = "response") > 0.5, 1, 0)
MSE_df12_lasso <- mean((pred - tst12$y)^2)
MSE_df12_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df12, trn12$y, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df12 <- glmnet(X_df12, trn12$y, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df12

pred <- ifelse(predict(ridge_df12, newx = Xtst_df12, type = "response") > 0.5, 1, 0)
MSE_df12_ridge <- mean((pred - tst12$y)^2)
MSE_df12_ridge
```

```{r}
NBayes_df12 <- naiveBayes(y ~ x4 + x6 + x9, data = trn12)
NBayes_df12
MSE_df12_NBayes <- mean((as.numeric(predict(NBayes_df12, tst12, type = "class"))-1-tst12$y)^2)
MSE_df12_NBayes
```



```{r}
lda_df12 <- lda(y ~ ns(x6) + ns(x9), data = trn12)
lda_df12
MSE_df12_lda <- mean((as.numeric(predict(lda_df12, tst12)$class) - 1 - tst12$y)^2)
MSE_df12_lda

qda_df12 <- qda(y ~ ns(x6) + ns(x9), data = trn12)
qda_df12
MSE_df12_qda <- mean((as.numeric(predict(qda_df12, tst12)$class) - 1 - tst12$y)^2)
MSE_df12_qda
```


```{r}
MSE_df12_lasso
MSE_df12_ridge
MSE_df12_NBayes
MSE_df12_lda
MSE_df12_qda

final_csv[,13] <- predict(lda_df12, newdata = df12_test)$class
```



# dataset 13


```{r}
df13_train <- read.csv("./dataset/dataset13_train.csv")
df13_test <- read.csv("./dataset/dataset13_test.csv")
df13_train
summary(df13_train$y)
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = df13_train)
ggcorrplot(cor(df13_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df13 <- initial_split(df13_train, prop = 0.9)
trn13 <- training(data_split_df13)
tst13 <- testing(data_split_df13)
```


```{r}
X_df13 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn13)[, -1]
Xtst_df13 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = tst13)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df13, trn13$y, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df13 <- glmnet(X_df13, trn13$y, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df13
coef(lasso_df13)

pred <- ifelse(predict(lasso_df13, newx = Xtst_df13, type = "response") > 0.5, 1, 0)
MSE_df13_lasso <- mean((pred - tst13$y)^2)
MSE_df13_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df13, trn13$y, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df13 <- glmnet(X_df13, trn13$y, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df13

pred <- ifelse(predict(ridge_df13, newx = Xtst_df13, type = "response") > 0.5, 1, 0)
MSE_df13_ridge <- mean((pred - tst13$y)^2)
MSE_df13_ridge
```


```{r}
NBayes_df13 <- naiveBayes(y ~ x1 + x2 + x4 + x5 + x6 + x8, data = trn13)
NBayes_df13
MSE_df13_NBayes <- mean((as.numeric(predict(NBayes_df13, tst13, type = "class"))-1-tst13$y)^2)
MSE_df13_NBayes
```


```{r}
MSE_df13_lasso
MSE_df13_ridge
MSE_df13_NBayes

final_csv[, 14] <- predict(NBayes_df13, df13_test, type = "class")
```



# dataset 14


```{r}
df14_train <- read.csv("./dataset/dataset14_train.csv")
df14_test <- read.csv("./dataset/dataset14_test.csv")
df14_train
summary(df14_train$y)
ggcorrplot(cor(df14_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df14 <- initial_split(df14_train, prop = 0.9)
trn14 <- training(data_split_df14)
tst14 <- testing(data_split_df14)
```






```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn14)

best_select_df14 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn14, nvmax = 10)
sum_lm14 <- summary(best_select_df14)
coef(best_select_df14, id = which.min(sum_lm14$cp))
coef(best_select_df14, id = which.min(sum_lm14$bic))
coef(best_select_df14, id = which.min(sum_lm14$adjr2))
```





```{r}
lm1_df14 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x10, data = trn14)

MSE_df14_lm1 <- mean(pluck((predict(lm1_df14, tst14) - tst14$y)^2, 1))
MSE_df14_lm1

lm2_df14 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x1 + x2 + x4 + x10, data = trn14)

MSE_df14_lm2 <- mean(pluck((predict(lm2_df14, tst14) - tst14$y)^2, 1))
MSE_df14_lm2
```



```{r}
gam.full_df14 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn14)
summary(gam.full_df14)
```



```{r}
gam_df14 <- gam(y ~ x1 + x3 + x5 + ns(x8) + x10, data = trn14)
summary(gam_df14)

MSE_df14_gam <- mean(pluck((predict(gam_df14, newdata = tst14) - tst14$y)^2, 1))
MSE_df14_gam
```


```{r}
X_df14 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn14)[,-1]
Xtst_df14 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst14)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df14, trn14$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df14 <- glmnet(X_df14, trn14$y, alpha = 1, lambda = bestlambda)
summary(lasso_df14)
MSE_df14_lasso <- mean((predict(lasso_df14, Xtst_df14) - tst14$y)^2)
MSE_df14_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df14, trn14$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df14 <- glmnet(X_df14, trn14$y, alpha = 0, lambda = bestlambda)
summary(ridge_df14)
MSE_df14_ridge <- mean((predict.glmnet(ridge_df14, Xtst_df14) - tst14$y)^2)
MSE_df14_ridge
```


```{r}
MSE_df14_lm1
MSE_df14_lm2
MSE_df14_gam
MSE_df14_lasso
MSE_df14_ridge

final_csv[,15] <- predict(gam_df14, newdata = df14_test)
```





# dataset 15


```{r}
df15_train <- read.csv("./dataset/dataset15_train.csv")
df15_test <- read.csv("./dataset/dataset15_test.csv")
df15_train
summary(df15_train$y)
ggcorrplot(cor(df15_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df15 <- initial_split(df15_train, prop = 0.9)
trn15 <- training(data_split_df15)
tst15 <- testing(data_split_df15)
```



```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn15)

best_select_df15 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn15, nvmax = 10)
sum_lm15 <- summary(best_select_df15)
coef(best_select_df15, id = which.min(sum_lm15$cp))
coef(best_select_df15, id = which.min(sum_lm15$bic))
coef(best_select_df15, id = which.min(sum_lm15$adjr2))
```





```{r}
lm1_df15 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x6, data = trn15)

MSE_df15_lm1 <- mean(pluck((predict(lm1_df15, tst15) - tst15$y)^2, 1))
MSE_df15_lm1


lm2_df15 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x5 + x6, data = trn15)

MSE_df15_lm2 <- mean(pluck((predict(lm2_df15, tst15) - tst15$y)^2, 1))
MSE_df15_lm2


lm3_df15 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x4 + x5 + x6 + x8, data = trn15)

MSE_df15_lm3 <- mean(pluck((predict(lm3_df15, tst15) - tst15$y)^2, 1))
MSE_df15_lm3
```



```{r}
gam.full_df15 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn15)
summary(gam.full_df15)
```



```{r}
gam1_df15 <- gam(y ~ x4 + x5 + ns(x8), data = trn15)
summary(gam1_df15)
gam2_df15 <- gam(y ~ x4 + x5 + ns(x6) + ns(x8), data = trn15)
summary(gam2_df15)

MSE_df15_gam1 <- mean(pluck((predict(gam1_df15, newdata = tst15) - tst15$y)^2, 1))
MSE_df15_gam1
MSE_df15_gam2 <- mean(pluck((predict(gam2_df15, newdata = tst15) - tst15$y)^2, 1))
MSE_df15_gam2
```



```{r}
X_df15 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn15)[,-1]
Xtst_df15 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst15)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df15, trn15$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df15 <- glmnet(X_df15, trn15$y, alpha = 1, lambda = bestlambda)
summary(lasso_df15)
MSE_df15_lasso <- mean((predict(lasso_df15, Xtst_df15) - tst15$y)^2)
MSE_df15_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df15, trn15$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df15 <- glmnet(X_df15, trn15$y, alpha = 0, lambda = bestlambda)
summary(ridge_df15)
MSE_df15_ridge <- mean((predict.glmnet(ridge_df15, Xtst_df15) - tst15$y)^2)
MSE_df15_ridge
```


```{r}
MSE_df15_lm1
MSE_df15_lm2
MSE_df15_lm3
MSE_df15_gam1
MSE_df15_gam2
MSE_df15_lasso
MSE_df15_ridge

final_csv[,16] <- predict(gam2_df15, newdata = df15_test)
```


왠지 모르게 일반적인 LM을 쓸 이유가 없고 GAM만 줄창 쓰게 되는 듯...? 애초에 MSE만 줄이는 게 목표라서 LM의 가장 큰 장점인 '해석의 용이성'은 별로 중요한 문제가 아니고 GAM이 LM에 비해 단점이 하나도 없는 상황에 가깝다. Ridge와 Lasso가 퍼포먼스가 좋지 않은 것도 지금 선형 Ridge와 Lasso만 써서 그렇다.



# dataset 16


```{r}
df16_train <- read.csv("./dataset/dataset16_train.csv")
df16_test <- read.csv("./dataset/dataset16_test.csv")
df16_train
summary(df16_train$y)
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = df16_train)
ggcorrplot(cor(df16_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df16 <- initial_split(df16_train, prop = 0.9)
trn16 <- training(data_split_df16)
tst16 <- testing(data_split_df16)
```


```{r}
X_df16 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn16)[, -1]
Xtst_df16 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = tst16)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df16, trn16$y, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df16 <- glmnet(X_df16, trn16$y, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df16
coef(lasso_df16)

pred <- ifelse(predict(lasso_df16, newx = Xtst_df16, type = "response") > 0.5, 1, 0)
MSE_df16_lasso <- mean((pred - tst16$y)^2)
MSE_df16_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df16, trn16$y, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df16 <- glmnet(X_df16, trn16$y, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df16

pred <- ifelse(predict(ridge_df16, newx = Xtst_df16, type = "response") > 0.5, 1, 0)
MSE_df16_ridge <- mean((pred - tst16$y)^2)
MSE_df16_ridge
```


```{r}
NBayes_df16 <- naiveBayes(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn16)
NBayes_df16
MSE_df16_NBayes <- mean((as.numeric(predict(NBayes_df16, tst16, type = "class"))-1-tst16$y)^2)
MSE_df16_NBayes
```


```{r}
MSE_df16_lasso
MSE_df16_ridge
MSE_df16_NBayes

final_csv[, 17] <- predict(NBayes_df16, df16_test, type = "class")
```


왜 뭔 모델을 돌리든 MSE가 25%지....



# dataset 17


```{r}
df17_train <- read.csv("./dataset/dataset17_train.csv")
df17_test <- read.csv("./dataset/dataset17_test.csv")
df17_train
summary(df17_train$y)
ggcorrplot(cor(df17_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df17 <- initial_split(df17_train, prop = 0.9)
trn17 <- training(data_split_df17)
tst17 <- testing(data_split_df17)
```


```{r}
mean(trn17$y)
var(trn17$y)
```


```{r}
zipo1_df17 <- zeroinfl(y ~ x1 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn17)
summary(zipo1_df17)
MSE_df17_zipo1 <- mean((round(predict(zipo1_df17, newdata = tst17, type = 'count')) - tst17$y)^2)

zipo2_df17 <- zeroinfl(y ~ x4 + x6 + x8 + x10, data = trn17)
summary(zipo2_df17)
MSE_df17_zipo2 <- mean((round(predict(zipo2_df17, newdata = tst17, type = 'count')) - tst17$y)^2)
```


```{r}
MSE_df17_zipo1 
MSE_df17_zipo2 

final_csv[,18] <- round(predict(zipo1_df17, newdata = df17_test, type = 'count'))
```



# dataset 18


```{r}
df18_train <- read.csv("./dataset/dataset18_train.csv")
df18_test <- read.csv("./dataset/dataset18_test.csv")
df18_train
summary(df18_train$y)
ggcorrplot(cor(df18_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df18 <- initial_split(df18_train, prop = 0.9)
trn18 <- training(data_split_df18)
tst18 <- testing(data_split_df18)
```




```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn18)
gam.full_df18 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn18)
summary(gam.full_df18)
```



```{r}
gam1_df18 <- gam(y ~ x1 + x2 + x3 + x5 + ns(x6) + ns(x7) + ns(x8), data = trn18)
summary(gam1_df18)
gam2_df18 <- gam(y ~ x1 + x2 + x5 + ns(x6) + ns(x7) + ns(x8), data = trn18)
summary(gam2_df18)
gam3_df18 <- gam(y ~ x1 + x3 + x5 + ns(x6) + ns(x7) + ns(x8), data = trn18)
summary(gam3_df18)
gam4_df18 <- gam(y ~ x1 + x5 + ns(x6) + ns(x7) + ns(x8), data = trn18)
summary(gam4_df18)

MSE_df18_gam1 <- mean(pluck((predict(gam1_df18, newdata = tst18) - tst18$y)^2, 1))
MSE_df18_gam1
MSE_df18_gam2 <- mean(pluck((predict(gam2_df18, newdata = tst18) - tst18$y)^2, 1))
MSE_df18_gam2
MSE_df18_gam3 <- mean(pluck((predict(gam3_df18, newdata = tst18) - tst18$y)^2, 1))
MSE_df18_gam3
MSE_df18_gam4 <- mean(pluck((predict(gam4_df18, newdata = tst18) - tst18$y)^2, 1))
MSE_df18_gam4
```


```{r}
X_df18 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn18)[,-1]
Xtst_df18 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst18)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df18, trn18$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df18 <- glmnet(X_df18, trn18$y, alpha = 1, lambda = bestlambda)
summary(lasso_df18)
MSE_df18_lasso <- mean((predict(lasso_df18, Xtst_df18) - tst18$y)^2)
MSE_df18_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df18, trn18$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df18 <- glmnet(X_df18, trn18$y, alpha = 0, lambda = bestlambda)
summary(ridge_df18)
MSE_df18_ridge <- mean((predict.glmnet(ridge_df18, Xtst_df18) - tst18$y)^2)
MSE_df18_ridge
```


```{r}
MSE_df18_gam1
MSE_df18_gam2
MSE_df18_gam3
MSE_df18_gam4
MSE_df18_lasso
MSE_df18_ridge

final_csv[,19] <- predict(gam4_df18, newdata = df18_test)
```






# dataset 19


```{r}
df19_train <- read.csv("./dataset/dataset19_train.csv")
df19_test <- read.csv("./dataset/dataset19_test.csv")
df19_train
summary(df19_train$y)
ggcorrplot(cor(df19_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df19 <- initial_split(df19_train, prop = 0.9)
trn19 <- training(data_split_df19)
tst19 <- testing(data_split_df19)
```



```{r}
pairs(~x6 + x7 + x8 + x9 + x10 + y, data = trn19)
gam.full_df19 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9) + ns(x10), data = trn19)
summary(gam.full_df19)
```



```{r}
gam1_df19 <- gam(y ~ x1 + x2 + x3 + x4 + ns(x6) + ns(x7) + ns(x8) + ns(x9), data = trn19)
summary(gam1_df19)
gam2_df19 <- gam(y ~ x1 + x2 + x3 + x4 + x5 + ns(x6) + ns(x7) + ns(x8) + ns(x9), data = trn19)
summary(gam2_df19)

MSE_df19_gam1 <- mean(pluck((predict(gam1_df19, newdata = tst19) - tst19$y)^2, 1))
MSE_df19_gam1
MSE_df19_gam2 <- mean(pluck((predict(gam2_df19, newdata = tst19) - tst19$y)^2, 1))
MSE_df19_gam2
```


```{r}
X_df19 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn19)[,-1]
Xtst_df19 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst19)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df19, trn19$y, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df19 <- glmnet(X_df19, trn19$y, alpha = 1, lambda = bestlambda)
summary(lasso_df19)
MSE_df19_lasso <- mean((predict(lasso_df19, Xtst_df19) - tst19$y)^2)
MSE_df19_lasso


set.seed(42)
cv.out <- cv.glmnet(X_df19, trn19$y, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df19 <- glmnet(X_df19, trn19$y, alpha = 0, lambda = bestlambda)
summary(ridge_df19)
MSE_df19_ridge <- mean((predict.glmnet(ridge_df19, Xtst_df19) - tst19$y)^2)
MSE_df19_ridge
```


```{r}
MSE_df19_gam1
MSE_df19_gam2
MSE_df19_lasso
MSE_df19_ridge

tstX <- model.matrix(~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df19_test)[, -1]
final_csv[,20] <- predict(lasso_df19, newx = tstX)[,1]
```





# dataset 20


```{r}
df20_train <- read.csv("./dataset/dataset20_train.csv")
df20_test <- read.csv("./dataset/dataset20_test.csv")
df20_train
summary(df20_train$y)
ggcorrplot(cor(df20_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df20 <- initial_split(df20_train, prop = 0.9)
trn20 <- training(data_split_df20)
tst20 <- testing(data_split_df20)
```


```{r}
mean(trn20$y)
var(trn20$y)
```

count data.

```{r}
zinegbin1_df20 <- zeroinfl(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn20)
summary(zinegbin1_df20)
MSE_df20_zinegbin1 <- mean((round(predict(zinegbin1_df20, newdata = tst20, type = 'count')) - tst20$y)^2)

zinegbin2_df20 <- zeroinfl(y ~ x3 + x4 + x5 + x9, data = trn20)
summary(zinegbin2_df20)
MSE_df20_zinegbin2 <- mean((round(predict(zinegbin2_df20, newdata = tst20, type = 'count')) - tst20$y)^2)

MSE_df20_zinegbin1
MSE_df20_zinegbin2

final_csv[,21] <- round(predict(zinegbin1_df20, newdata = df20_test, type = 'count'))
```



```{r}
final_csv <- set_names(final_csv, colnames_final)
write.csv(final_csv, file = 'test_prediction.csv', row.names = TRUE)
```

